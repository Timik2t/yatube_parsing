# Yatube Parser

## Описание

Учебный проект для практики создания асинхронных парсеров, работы во фреймворке Scrapy и с библиотекой SQLAlchemy.

Парсится тестовый сайт по адресу: http://51.250.32.185/

В проекте реализована работа двух пауков:
- **yatube** - осуществляет поиск на всех страницах сайта постов, написанных в понедельник, создает базу данных sqlite и заполняет её информацией из найденных постов.
- **group** - осуществляет поиск на всех страницах сайта ссылок на существующие группы, переходит на страницы групп и выводит в файл .csv информацию о группе и количество постов в ней.

## Ключевые технологии и библиотеки:
- [Python](https://www.python.org/)
- [Scrapy](https://pypi.org/project/Scrapy/)
- [SQLAlchemy](https://pypi.org/project/SQLAlchemy/)

## Подготовка к запуску

1. Склонируйте репозиторий на локальную машину:

    ```bash
    git clone git@github.com:Timik2t/yatube_parsing.git
    ```

2. Создайте и активируйте виртуальное окружение:

    ```bash
    python -m venv venv
    ```

    Активация окружения
    ```bash
    # Windows
    source venv/Scripts/activate
    ```
    ```bash
    # Linux
    source venv/bin/activate
    ```
3. Установите зависимости:

    ```bash
    pip install -r requirements.txt
    ```

## Запуск и управление:
- Запуск паука **yatube**:
```bash
scrapy crawl yatube
```
Будет создана база данных sqlite.db и заполнена только постами, написанными в понедельник.

---

- Запуск паука **group**:
```bash
scrapy crawl group -O <имя_файла>.csv
```
Будет создан файл <имя_файла>.csv с полученными данными.

---

## Альтернативный запуск паука **yatube** для сбора данных обо всех постах:
1. В файле settings.py необходимо закомментировать строки:
    ```bash
    ITEM_PIPELINES = {
    'yatube_parsing.pipelines.MondayPipeline': 300,
    }
    ```
2. Сохраните файл;
3. Выполните команду:
    ```bash
    scrapy crawl yatube -O <имя_файла>.csv
    ```
Будет создан файл <имя_файла>.csv с данными из всех постов на сайте.

---

### Автор
[Tim](https://github.com/Timik2t)
